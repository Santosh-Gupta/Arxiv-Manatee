{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "BetterBatchProcessAbstractSummaryDataSet.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Santosh-Gupta/Arxiv-Manatee/blob/master/BetterBatchProcessAbstractSummaryDataSet.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1V-sB7H2HCAV",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1143
        },
        "outputId": "fed15b9e-4b34-40a2-8570-a5ea063f2f9d"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')\n",
        "\n",
        "import zipfile\n",
        "import os\n",
        "import shutil\n",
        "import sys\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import gc\n",
        "# import pickle\n",
        "import json\n",
        "import shutil\n",
        "import threading\n",
        "import time\n",
        "from IPython.utils import io\n",
        "\n",
        "import requests\n",
        "import time\n",
        "import urllib.request\n",
        "import datetime\n",
        "\n",
        "!pip install fastparquet\n",
        "\n",
        "!apt-get install libmagic-dev\n",
        "!pip install python-magic\n",
        "import magic\n",
        "mime = magic.Magic(mime=True)\n",
        "\n",
        "from concurrent.futures import ThreadPoolExecutor\n"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3Aietf%3Awg%3Aoauth%3A2.0%3Aoob&scope=email%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdocs.test%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive.photos.readonly%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fpeopleapi.readonly&response_type=code\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/gdrive\n",
            "Collecting fastparquet\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/85/b9/dc59386bc5824f86c640e7178fc78986f0c81763b924b2e37337ffb6a563/fastparquet-0.3.1.tar.gz (149kB)\n",
            "\u001b[K     |████████████████████████████████| 153kB 4.9MB/s \n",
            "\u001b[?25hRequirement already satisfied: pandas>=0.19 in /usr/local/lib/python3.6/dist-packages (from fastparquet) (0.24.2)\n",
            "Requirement already satisfied: numba>=0.28 in /usr/local/lib/python3.6/dist-packages (from fastparquet) (0.40.1)\n",
            "Requirement already satisfied: numpy>=1.11 in /usr/local/lib/python3.6/dist-packages (from fastparquet) (1.16.4)\n",
            "Collecting thrift>=0.11.0 (from fastparquet)\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/c6/b4/510617906f8e0c5660e7d96fbc5585113f83ad547a3989b80297ac72a74c/thrift-0.11.0.tar.gz (52kB)\n",
            "\u001b[K     |████████████████████████████████| 61kB 20.7MB/s \n",
            "\u001b[?25hRequirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from fastparquet) (1.12.0)\n",
            "Requirement already satisfied: pytz>=2011k in /usr/local/lib/python3.6/dist-packages (from pandas>=0.19->fastparquet) (2018.9)\n",
            "Requirement already satisfied: python-dateutil>=2.5.0 in /usr/local/lib/python3.6/dist-packages (from pandas>=0.19->fastparquet) (2.5.3)\n",
            "Requirement already satisfied: llvmlite>=0.25.0dev0 in /usr/local/lib/python3.6/dist-packages (from numba>=0.28->fastparquet) (0.29.0)\n",
            "Building wheels for collected packages: fastparquet, thrift\n",
            "  Building wheel for fastparquet (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Stored in directory: /root/.cache/pip/wheels/f3/27/fb/839c776ec8689ff9ee52ad3e91d7d4d848ac6d7545a127d5b0\n",
            "  Building wheel for thrift (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Stored in directory: /root/.cache/pip/wheels/be/36/81/0f93ba89a1cb7887c91937948519840a72c0ffdd57cac0ae8f\n",
            "Successfully built fastparquet thrift\n",
            "Installing collected packages: thrift, fastparquet\n",
            "Successfully installed fastparquet-0.3.1 thrift-0.11.0\n",
            "Reading package lists... Done\n",
            "Building dependency tree       \n",
            "Reading state information... Done\n",
            "The following package was automatically installed and is no longer required:\n",
            "  libnvidia-common-410\n",
            "Use 'apt autoremove' to remove it.\n",
            "The following additional packages will be installed:\n",
            "  libmagic-mgc libmagic1\n",
            "Suggested packages:\n",
            "  file\n",
            "The following NEW packages will be installed:\n",
            "  libmagic-dev libmagic-mgc libmagic1\n",
            "0 upgraded, 3 newly installed, 0 to remove and 16 not upgraded.\n",
            "Need to get 332 kB of archives.\n",
            "After this operation, 5,549 kB of additional disk space will be used.\n",
            "Get:1 http://archive.ubuntu.com/ubuntu bionic-updates/main amd64 libmagic-mgc amd64 1:5.32-2ubuntu0.2 [184 kB]\n",
            "Get:2 http://archive.ubuntu.com/ubuntu bionic-updates/main amd64 libmagic1 amd64 1:5.32-2ubuntu0.2 [68.5 kB]\n",
            "Get:3 http://archive.ubuntu.com/ubuntu bionic-updates/main amd64 libmagic-dev amd64 1:5.32-2ubuntu0.2 [79.5 kB]\n",
            "Fetched 332 kB in 1s (622 kB/s)\n",
            "Selecting previously unselected package libmagic-mgc.\n",
            "(Reading database ... 130942 files and directories currently installed.)\n",
            "Preparing to unpack .../libmagic-mgc_1%3a5.32-2ubuntu0.2_amd64.deb ...\n",
            "Unpacking libmagic-mgc (1:5.32-2ubuntu0.2) ...\n",
            "Selecting previously unselected package libmagic1:amd64.\n",
            "Preparing to unpack .../libmagic1_1%3a5.32-2ubuntu0.2_amd64.deb ...\n",
            "Unpacking libmagic1:amd64 (1:5.32-2ubuntu0.2) ...\n",
            "Selecting previously unselected package libmagic-dev:amd64.\n",
            "Preparing to unpack .../libmagic-dev_1%3a5.32-2ubuntu0.2_amd64.deb ...\n",
            "Unpacking libmagic-dev:amd64 (1:5.32-2ubuntu0.2) ...\n",
            "Setting up libmagic-mgc (1:5.32-2ubuntu0.2) ...\n",
            "Setting up libmagic1:amd64 (1:5.32-2ubuntu0.2) ...\n",
            "Processing triggers for libc-bin (2.27-3ubuntu1) ...\n",
            "Processing triggers for man-db (2.8.3-2ubuntu0.1) ...\n",
            "Setting up libmagic-dev:amd64 (1:5.32-2ubuntu0.2) ...\n",
            "Collecting python-magic\n",
            "  Downloading https://files.pythonhosted.org/packages/42/a1/76d30c79992e3750dac6790ce16f056f870d368ba142f83f75f694d93001/python_magic-0.4.15-py2.py3-none-any.whl\n",
            "Installing collected packages: python-magic\n",
            "Successfully installed python-magic-0.4.15\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9-ISQEGoHPoQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "introStrings = ['introduction:' , 'case:' , 'introduction' , 'case' ]\n",
        "backgroundStrins = ['literature:' , 'background:',  'related:' , 'literature' , 'background',  'related' ]\n",
        "methodStrings = [ 'methods:' , 'method:', 'techniques:', 'methodology:' , 'methods' , 'method', 'techniques', 'methodology',  'experimental:', 'experiments:', 'experiment:', 'experimental', 'experiments', 'experiment']\n",
        "resultStrings = [ 'results:', 'result:', 'results', 'result']\n",
        "discussioStrings = [ 'discussion:' , 'limitations:'  , 'discussion' , 'limitations']\n",
        "conclusionStrings = ['conclusion:' , 'conclusions:', 'concluding:' , 'conclusion' , 'conclusions', 'concluding' ]\n",
        "\n",
        "allStrings = [ introStrings, backgroundStrins, methodStrings, resultStrings, discussioStrings, conclusionStrings ]\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ejEHDnTgHQ_Q",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 504
        },
        "outputId": "8fc4034f-1348-488c-e6f1-542141cb0bd7"
      },
      "source": [
        "#Science parser stuff\n",
        "\n",
        "# Get cli\n",
        "!wget https://github.com/allenai/science-parse/releases/download/v2.0.3/science-parse-cli-assembly-2.0.3.jar\n",
        "\n",
        "# install wget\n",
        "!pip install wget\n",
        "\n",
        "# Install Java\n",
        "!apt-get install -y openjdk-8-jdk-headless -qq > /dev/null\n",
        "os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-8-openjdk-amd64\"\n",
        "!java -version"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2019-06-24 01:39:45--  https://github.com/allenai/science-parse/releases/download/v2.0.3/science-parse-cli-assembly-2.0.3.jar\n",
            "Resolving github.com (github.com)... 192.30.253.113\n",
            "Connecting to github.com (github.com)|192.30.253.113|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://github-production-release-asset-2e65be.s3.amazonaws.com/41012501/7bc09d80-f890-11e8-97d1-ad6f51e75175?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=AKIAIWNJYAX4CSVEH53A%2F20190624%2Fus-east-1%2Fs3%2Faws4_request&X-Amz-Date=20190624T013946Z&X-Amz-Expires=300&X-Amz-Signature=160b2fc956f1c08f5ca13002871dc1dfc70abeeb329d8fbf2cc5a9ffd09fe279&X-Amz-SignedHeaders=host&actor_id=0&response-content-disposition=attachment%3B%20filename%3Dscience-parse-cli-assembly-2.0.3.jar&response-content-type=application%2Foctet-stream [following]\n",
            "--2019-06-24 01:39:46--  https://github-production-release-asset-2e65be.s3.amazonaws.com/41012501/7bc09d80-f890-11e8-97d1-ad6f51e75175?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=AKIAIWNJYAX4CSVEH53A%2F20190624%2Fus-east-1%2Fs3%2Faws4_request&X-Amz-Date=20190624T013946Z&X-Amz-Expires=300&X-Amz-Signature=160b2fc956f1c08f5ca13002871dc1dfc70abeeb329d8fbf2cc5a9ffd09fe279&X-Amz-SignedHeaders=host&actor_id=0&response-content-disposition=attachment%3B%20filename%3Dscience-parse-cli-assembly-2.0.3.jar&response-content-type=application%2Foctet-stream\n",
            "Resolving github-production-release-asset-2e65be.s3.amazonaws.com (github-production-release-asset-2e65be.s3.amazonaws.com)... 52.216.21.107\n",
            "Connecting to github-production-release-asset-2e65be.s3.amazonaws.com (github-production-release-asset-2e65be.s3.amazonaws.com)|52.216.21.107|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 62867954 (60M) [application/octet-stream]\n",
            "Saving to: ‘science-parse-cli-assembly-2.0.3.jar’\n",
            "\n",
            "science-parse-cli-a 100%[===================>]  59.96M  28.3MB/s    in 2.1s    \n",
            "\n",
            "2019-06-24 01:39:48 (28.3 MB/s) - ‘science-parse-cli-assembly-2.0.3.jar’ saved [62867954/62867954]\n",
            "\n",
            "Collecting wget\n",
            "  Downloading https://files.pythonhosted.org/packages/47/6a/62e288da7bcda82b935ff0c6cfe542970f04e29c756b0e147251b2fb251f/wget-3.2.zip\n",
            "Building wheels for collected packages: wget\n",
            "  Building wheel for wget (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Stored in directory: /root/.cache/pip/wheels/40/15/30/7d8f7cea2902b4db79e3fea550d7d7b85ecb27ef992b618f3f\n",
            "Successfully built wget\n",
            "Installing collected packages: wget\n",
            "Successfully installed wget-3.2\n",
            "openjdk version \"11.0.3\" 2019-04-16\n",
            "OpenJDK Runtime Environment (build 11.0.3+7-Ubuntu-1ubuntu218.04.1)\n",
            "OpenJDK 64-Bit Server VM (build 11.0.3+7-Ubuntu-1ubuntu218.04.1, mixed mode, sharing)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dKm3VPFjPPOV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "if not os.path.exists('ArxivPDFS'):\n",
        "    os.mkdir('ArxivPDFS')\n",
        "    \n",
        "if not os.path.exists('scienceParserOutput'):\n",
        "    os.mkdir('scienceParserOutput')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pE5eBwIvPLv5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def DownloadPaper( paperNumber, testDF, n ):\n",
        "    sema.acquire()\n",
        "    \n",
        "    links = []\n",
        "    links.append( testDF['s2PdfUrl'].iloc[n] )\n",
        "    for item in testDF['pdfUrls'].iloc[n]:\n",
        "        links.append( item )\n",
        "\n",
        "    for linkAddress in links: #no, we need to end afte a sucuessful download. And also need to check if the file downloaded is in indeed a pdf and over 50 kbs \n",
        "        if not os.path.exists( 'ArxivPDFS/'+str(paperNumber)+'.pdf' ):\n",
        "            if not linkAddress == '':\n",
        "                try:\n",
        "                    urllib.request.urlretrieve( linkAddress ,  'ArxivPDFS/'+str(paperNumber)+'.pdf'  )\n",
        "                    if mime.from_file( 'ArxivPDFS/'+str(paperNumber)+'.pdf'  ) == 'application/pdf' and os.path.getsize(  'ArxivPDFS/'+str(paperNumber)+'.pdf'  ) > 25000 :   #check if file is pdf, and if pdf has at least 1 page \n",
        "                        break\n",
        "                    else:\n",
        "                        os.remove( 'ArxivPDFS/'+str(paperNumber)+'.pdf' )\n",
        "                except:\n",
        "                    pass \n",
        "        elif os.path.exists(  'ArxivPDFS/'+str(paperNumber)+'.pdf'  ) and not linkAddress == '' :  #if there already is a file, want it to exist, and take into account if just the directly is listed with no link \n",
        "            break\n",
        "    \n",
        "    sema.release()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FWFdOZypPNGs",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "file2C = 'gdrive/My Drive/ArxivAbstractSplitFiles6-18-19/AbstractsParsed{:02}.parquet.gzip'\n",
        "saveFormat = 'gdrive/My Drive/SemanticScholarAbstractSectionSummaryDataSet/AbstractSummaryDataSet{:02}.parquet.gzip'"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-_p_kVGSPLs7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def DownloadAllPapers(k, testDF0):\n",
        "\n",
        "    totalNumberOfRows = testDF0.shape[0]\n",
        "    \n",
        "    for i in range(0, totalNumberOfRows): #totalNumberOfRows\n",
        "        folderNumber = testDF0.iloc[i]['PaperID']\n",
        "        if (i+1)%10000 == 0:\n",
        "            print( 'i is', i)\n",
        "            time.sleep(360)\n",
        "        thread = threading.Thread(target=DownloadPaper,args=(folderNumber, testDF0, i ))\n",
        "        thread.start()\n",
        "    \n",
        "    print(len(os.listdir('ArxivPDFS')))\n",
        "    time.sleep(120)\n",
        "\n",
        "    for i in range(0, totalNumberOfRows): #totalNumberOfRows\n",
        "        folderNumber = testDF0.iloc[i]['PaperID']\n",
        "        if (i+1)%10000 == 0:\n",
        "            time.sleep(120)\n",
        "            print( 'i is', i)\n",
        "        thread = threading.Thread(target=DownloadPaper,args=(folderNumber, testDF0, i ))\n",
        "        thread.start()\n",
        "\n",
        "    print(len(os.listdir('ArxivPDFS')))\n",
        "    time.sleep(120)\n",
        "\n",
        "    for i in range(0, totalNumberOfRows): #totalNumberOfRows\n",
        "        folderNumber = testDF0.iloc[i]['PaperID']\n",
        "        if (i+1)%10000 == 0:\n",
        "            time.sleep(120)\n",
        "            print( 'i is', i)\n",
        "        thread = threading.Thread(target=DownloadPaper,args=(folderNumber, testDF0, i ))\n",
        "        thread.start()\n",
        "            \n",
        "    print(len(os.listdir('ArxivPDFS')))\n",
        "    time.sleep(120)\n",
        "\n",
        "#     for i in range(0, totalNumberOfRows): #totalNumberOfRows\n",
        "#         folderNumber = testDF0.iloc[i]['PaperID']\n",
        "#         if (i+1)%10000 == 0:\n",
        "#             time.sleep(120)\n",
        "#             print( 'i is', i)\n",
        "#         thread = threading.Thread(target=DownloadPaper,args=(folderNumber, testDF0, i ))\n",
        "#         thread.start()\n",
        "            \n",
        "#     print(len(os.listdir('ArxivPDFS')))\n",
        "#     time.sleep(120)\n",
        "\n",
        "#     for i in range(0, totalNumberOfRows): #totalNumberOfRows\n",
        "#         folderNumber = testDF0.iloc[i]['PaperID']\n",
        "#         if (i+1)%10000 == 0:\n",
        "#             time.sleep(60)\n",
        "#             print( 'i is', i)\n",
        "#         thread = threading.Thread(target=DownloadPaper,args=(folderNumber, testDF0, i ))\n",
        "#         thread.start()\n",
        "            \n",
        "#     print(len(os.listdir('ArxivPDFS')))\n",
        "#     time.sleep(60)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "87NfkXKHPLnF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Parsing all the pdfs\n",
        "\n",
        "%%capture\n",
        "\n",
        "def ScienceParse():\n",
        "    currentIn = 'ArxivPDFS'      \n",
        "    currentOut = 'scienceParserOutput'\n",
        "    \n",
        "    !java -Xmx6g -jar science-parse-cli-assembly-2.0.3.jar {currentIn} -o {currentOut}   "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w2wamwSkPHKj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Load the JSON, remove nulls/nones from JSON, match sectional text to summary\n",
        "\n",
        "#Alternative method to filtering out null and None values from JSON, in case first method fails. \n",
        "def replaceDD(data, search, replacement, parent=None, index=None):  \n",
        "    if data == search:\n",
        "        parent[index] = replacement\n",
        "    elif isinstance(data, (list, dict)):\n",
        "        for index, item in enumerate(data) if isinstance(data, list) else data.items():\n",
        "            replaceDD(item, search, replacement, parent=data, index=index)\n",
        "\n",
        "def MatchSummaries(testDF):\n",
        "    \n",
        "    totalNumberOfRows = testDF.shape[0]\n",
        "            \n",
        "    for i in range(0, totalNumberOfRows): #totalNumberOfRows\n",
        "\n",
        "        folderNumber = testDF.iloc[i]['PaperID']\n",
        "\n",
        "        if os.path.isfile('scienceParserOutput/' + str(folderNumber) + '.pdf.json') == True:\n",
        "            with open('scienceParserOutput/' + str(folderNumber) + '.pdf.json' ) as f:\n",
        "                parsed = json.load(f)\n",
        "                try:\n",
        "                    r = json.dumps(parsed).replace('null',  '\"NothingExistsHere\"' ) #remove all nulls and Nones and replace with empty strings\n",
        "                    parsed = json.loads(r)\n",
        "                except:\n",
        "                    replaceDD(parsed, None, 'NothingExistsHere')\n",
        "                    pass\n",
        "        else:\n",
        "            continue\n",
        "\n",
        "        whichList = testDF.iloc[i]['AbstractHeader']\n",
        "        for item in allStrings:\n",
        "            if whichList.lower() in item:\n",
        "                absSecsList = item\n",
        "                break\n",
        "\n",
        "        thisSectionText=[]\n",
        "        allSections = parsed['metadata']['sections']\n",
        "\n",
        "        j=0\n",
        "        whileCondition1 = True\n",
        "        whileCondition2 = True\n",
        "        while j < (len( allSections) ) and whileCondition1 == True :\n",
        "            try:\n",
        "                for headWord in allSections[j]['heading'].lower().split():\n",
        "                    if headWord in absSecsList:  #now, we have the first one. now, lets keep going until it reaches an item not in any list. \n",
        "                        thisSectionText.append( allSections[j]['heading'] ) \n",
        "                        thisSectionText.append( allSections[j]['text'] ) #we added the first one, lets keep adding sections until we reach one where that section is not on the list \n",
        "                        j=j+1   #go to the next section\n",
        "                        canAdd = True\n",
        "                        whileCondition = True\n",
        "                        while j < (len( allSections) ) and whileCondition2 == True: #if it goes into this while loop, then the outter while loop doesn't iterate. The outter while loop is only to find the first one. \n",
        "                            for item in allStrings: #goes though each of the catagory lists \n",
        "                                for headWord2 in allSections[j]['heading'].lower().split():\n",
        "                                    if headWord2 in item: #check if the next section's header is in any of the lists. If it is, it does Not belong to the current abstract text. Break out of all loops \n",
        "                                        if not item == absSecsList: #exclude the section list we're working on \n",
        "                                            #break out of all loops\n",
        "                                            canAdd = False\n",
        "                                            whileCondition2 = False\n",
        "                                            whileCondition1 = False\n",
        "                                            break #break \n",
        "                            if canAdd == True:\n",
        "                                thisSectionText.append( allSections[j]['heading'] ) \n",
        "                                thisSectionText.append( allSections[j]['text'] )\n",
        "                                j=j+1\n",
        "                                whileCondition2 = True\n",
        "                            if j > len( allSections ): #double check if it's less than , or just equal \n",
        "                                whileCondition2 = False\n",
        "                                whileCondition1 = False\n",
        "                        break #we just need to check if at least one word of header is in the abstract list . Breaks out of 'if headWord in absSecsList:'\n",
        "            except:\n",
        "                pass\n",
        "            j = j + 1 #check if next section is in abstract list \n",
        "\n",
        "        textToAppend = '. '.join(thisSectionText)                  \n",
        "        textToAppend = ''.join([m for m in textToAppend if not m.isdigit()])\n",
        "        textToAppend = ' '.join(textToAppend.split()[:512])\n",
        "\n",
        "        testDF.at[i, \"paperSection\"] = textToAppend\n",
        "        \n",
        "    return testDF"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0RCfTSsOPHF6",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 895
        },
        "outputId": "bd7e4a55-f655-4820-d33d-895666c6745c"
      },
      "source": [
        "print(datetime.datetime.now().time() )\n",
        "\n",
        "maxthreads = 200 #number of simultaneous downloads. May need to increase if there is a thread overload, or increase time between threads.  \n",
        "sema = threading.Semaphore(value=maxthreads)\n",
        "\n",
        "for mstr in range(0, 1): #there are 47 files \n",
        "    print('mstr at ', mstr)\n",
        "    \n",
        "    if os.path.exists('ArxivPDFS'):\n",
        "        shutil.rmtree('ArxivPDFS')\n",
        "    \n",
        "    if os.path.exists('scienceParserOutput'):\n",
        "        shutil.rmtree('scienceParserOutput')\n",
        "    \n",
        "    if not os.path.exists('ArxivPDFS'):\n",
        "        os.mkdir('ArxivPDFS')\n",
        "    \n",
        "    if not os.path.exists('scienceParserOutput'):\n",
        "        os.mkdir('scienceParserOutput')\n",
        "    \n",
        "    testDF0 = pd.read_parquet( file2C.format(mstr) , engine = 'fastparquet')\n",
        "    testDF0.columns = [ 'AbstractHeader', 'AbstractText', 'pdfUrls', 's2PdfUrl' , 's2Url' ]\n",
        "    testDF0['AbstractHeader'] = testDF0['AbstractHeader'].str.strip()\n",
        "    testDF0['PaperID'] = testDF0.groupby(['s2PdfUrl', 's2Url'], sort=False).ngroup()\n",
        "    testDF0[\"paperSection\"] = ''\n",
        "    numberofFolders = testDF0.tail(1).iloc[0]['PaperID'] \n",
        "    print('numberofFolders is ', numberofFolders)\n",
        "    totalNumberOfRows = testDF0.shape[0]\n",
        "        \n",
        "    print('Downloading')    \n",
        "    DownloadAllPapers(mstr, testDF0)\n",
        "    \n",
        "    print('Parsing. Longest Step, may take a few hours') \n",
        "    with io.capture_output() as captured: #Suppresses output \n",
        "        ScienceParse()\n",
        "    \n",
        "    print('Matching Summaries')\n",
        "    processedDF = MatchSummaries(testDF0)\n",
        "    \n",
        "    processedDF['Summary'] = processedDF['AbstractHeader'].str.cat(processedDF['AbstractText'], sep =\". \")\n",
        "    processedDF['Summary'] = processedDF['Summary'].str.replace('\\d+', '')\n",
        "    processedDF = processedDF[processedDF['paperSection']!='']\n",
        "    processedDF.drop('PaperID', axis=1, inplace=True)\n",
        "#     processedDF.drop('s2Url', axis=1, inplace=True)\n",
        "    processedDF.drop('s2PdfUrl', axis=1, inplace=True)\n",
        "    processedDF.drop('pdfUrls', axis=1, inplace=True)\n",
        "    processedDF.drop('AbstractHeader', axis=1, inplace=True)\n",
        "    processedDF.drop('AbstractText', axis=1, inplace=True)\n",
        "    processedDF.to_parquet( saveFormat.format(mstr) , engine = 'fastparquet' , compression = 'gzip', index = False)    \n",
        "\n",
        "print(datetime.datetime.now().time())"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "05:28:18.776557\n",
            "mstr at  0\n",
            "numberofFolders is  29628\n",
            "Downloading\n",
            "i is 9999\n",
            "i is 19999\n",
            "i is 29999\n",
            "i is 39999\n",
            "i is 49999\n",
            "i is 59999\n",
            "i is 69999\n",
            "i is 79999\n",
            "i is 89999\n",
            "i is 99999\n",
            "24847\n",
            "i is 9999\n",
            "i is 19999\n",
            "i is 29999\n",
            "i is 39999\n",
            "i is 49999\n",
            "i is 59999\n",
            "i is 69999\n",
            "i is 79999\n",
            "i is 89999\n",
            "i is 99999\n",
            "24850\n",
            "i is 9999\n",
            "i is 19999\n",
            "i is 29999\n",
            "i is 39999\n",
            "i is 49999\n",
            "i is 59999\n",
            "i is 69999\n",
            "i is 79999\n",
            "i is 89999\n",
            "i is 99999\n",
            "24850\n",
            "i is 9999\n",
            "i is 19999\n",
            "i is 29999\n",
            "i is 39999\n",
            "i is 49999\n",
            "i is 59999\n",
            "i is 69999\n",
            "i is 79999\n",
            "i is 89999\n",
            "i is 99999\n",
            "24849\n",
            "Parsing. Longest Step, may take a few hours\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "raJn8YZddUor",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "pd.set_option('max_colwidth', 3000)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NSBX4wlBfUlc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "processedDF.head(100)"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}